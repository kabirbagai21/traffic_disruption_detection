{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416bc9ea",
   "metadata": {
    "id": "1fae0120-1150-409d-afc9-a7390817a270"
   },
   "source": [
    "# Assignment 2: Rare Class Classification of Images - Mudd 2nd Floor\n",
    "## EECS E6691 2025 Spring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c68f6",
   "metadata": {
    "id": "db965b4b-6118-4879-8314-70961db81a09"
   },
   "source": [
    "## <font color=\"red\"><strong>TODO:</strong></font> [**REPLACE EVERYTHING INSIDE THE [] BRACKETS WITH YOUR KAGGLE TEAM NAME (See more details in the \"Evaluate your model (Kaggle)\") later**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793afee",
   "metadata": {
    "id": "28dd240c-935c-4783-b91f-46f92540a760"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590ff54",
   "metadata": {
    "id": "c1cc5100-516d-449b-b917-9c922ba7c510"
   },
   "source": [
    "<font color=\"red\"><strong>NOTE:</strong></font> This assignment can be done in groups of 2-3 students. Please form a team as soon as possible.\n",
    "\n",
    "You will work with a dataset of images of street traffic captured at the 120th Amsterdam Ave. and 120th Street intersection. Your task is to identify rare classes and situations that have potential to disrupt traffic flow or cause harm to pedestrians.\n",
    "\n",
    "You need to experiment with the dataset extensively and explore advanced deep learning methods and techniques. In particular, you will work on a \"multi-label\" classification task (also known as \"image tagging\") on a dataset more complex than your typical Kaggle or benchmark (such as Pascal-VOC and ImageNet) datasets.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> How you approach the classification problem and which models you use is rather **open-ended**, and we therefore **urge you to start early** and look for some potential methods of solving this assignment. You will need to do some literature search and search for models and methods online that suit our problem. Unlike introductory deep learning courses, simply loading and fine-tuning a CNN (such as a ResNet) from torchvision will likely not yield desirable results for this assignment.\n",
    "\n",
    "However, before anything else, we'll describe the dataset and problem in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776117d5",
   "metadata": {
    "id": "30b19675-63e3-44d2-81c4-9bfa65555dd2"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c54cdc",
   "metadata": {
    "id": "14417efa-5ca9-4f7f-b83c-2967091b7727"
   },
   "source": [
    "In this assignment we will work with an image dataset collected by Prof. Kostic's research group, acquired from a stationary video camera located on the 2nd floor of the Mudd building observing the intersection at of the 120th Street and Amsterdam Avenue. The dataset consists of 1002 images in $4K$ resolution ($width: 3840  \\times  length:2160$) captured in regular intervals throughout the day spanning from 12/22/2021 to 4/10/2022. This period encompasses several long-term traffic construction projects, providing scenes of stationary objects that occur in a continuous set of images. In addition to stationary objects, the images also contain a set of fine-grained objects and situations, such as \"off-road parking\", that are uncommon yet important to detect in everyday traffic settings. Combining these two sets yields highly imbalanced classes. Overall, the classes describe occurrences with potential to disrupt traffic flow or cause harm to pedestrians.\n",
    "\n",
    "This dataset can serve as a valuable asset for developing systems that can reliably foresee rare and dangerous street occurences and traffic events.\n",
    "\n",
    "\n",
    "Of the 1,002 images in the dataset, we have provided you the labels for 100 of them.  We keep the labels for the remaining 902 images hidden, for purposes of in-class Kaggle competition that you will participate in.\n",
    "\n",
    "\n",
    "The following table shows the frequency of classes within the provided \"training\" (or \"development\") set of 100 labeled images:\n",
    "|Class Index | Class Name | # Samples |\n",
    "| :--: | :- |:-: |\n",
    "| 0  |Barricade           |  97 |\n",
    "| 1  |Traffic cone        |  20 |\n",
    "| 2  |Traffic barrel      |  95 |\n",
    "| 3  |Scaffold            |  99 |\n",
    "| 4  |Trailer truck       |  11 |\n",
    "| 5  |Police car          |  5  |\n",
    "| 6  |Ambulance           |  3  |\n",
    "| 7  |Firecar             |  4  |\n",
    "| 8  |Excavator           |  16 |\n",
    "| 9  |Construction truck  |  8  |\n",
    "| 10  |Car moving truck   |  1  |\n",
    "| 11  |Offroad parking    |  3  |\n",
    "| 12  |Construction car   |  5  |\n",
    "| 13  |Construction worker|  3  |\n",
    "\n",
    "Although not completely identical, the distribution of labels of the hidden test set is similar to what is shown in the table above.\n",
    "\n",
    "\n",
    "There are several reasons as to why this dataset is  challenging to work with:\n",
    "* Firstly, notice in the table above that even though our provided dataset consists of 100 labeled images, the numbers in the \"# Samples\" column sum to over 100. This is because we are dealing with a \"multi-label\" classification problem, where each image may consist of one or more (or none) of the provided classes, making this a more challenging classification task than your typical \"multi-class\" classification problem. In the figure below, you can see an example from our dataset containing the “Barricade”, “Traffic barrel”, “Scaffold”, “Police car”, “Ambulance”, and “Excavator” classes.\n",
    "* Secondly, notice that the classes are highly imbalanced. Therefore, training a classifier on the provided data might be challenging due to some classes being seen much more frequent than others.\n",
    "* Thirdly, our images are of $4K$ resolution, which is higher than images in a typical dataset found online. Open-source models trained on datasets such as ImageNet, might therefore not work out-of-the-box when applied to our images directly. Furthermore, due to the high-resolution, there is a lot of fine-grained information throughout the image, which may not be captured by models if you simply downsample the image before feeeding to the model input.\n",
    "* The high-resolution also poses a challenge in that some classes are inherently smaller in size than others. Looking at the example image below, one would expect the \"ambulance\" and the \"police car\" to be easily identified due to their prominent size. On the other hand, classes such as \"traffic barrel\" are significantly more challenging to identify due to their small size.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./figures/example_image.jpg\" width=\"900\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cce7f9",
   "metadata": {
    "id": "6dd2444a-1dd3-4c89-9bed-a568c7f11da5"
   },
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48d92a",
   "metadata": {
    "id": "4ac1e2d7-ed96-4622-b270-6eb74cde0179"
   },
   "source": [
    "You will work with a multi-label classification problem (also referred to as \"image tagging\" in the literature). At a high-level, your task is to build a deep learning pipeline to predict the presence of the 14 classes enumerated above for the provided dataset. You need to  develop a processing pipeline using 100 labeled images and perform inference on 902 unlabeled images. You need to  submit your inference predictions through the in-class Kaggle competition. A part of your grade will be based on your performance in the Kaggle competition. Recall that in multi-label classification each image can have none,  one, or several classes present in the image. You will have to think about how to perform this kind of classification when designing your own deep learning pipeline.\n",
    "\n",
    "\n",
    "This assignment is **to be done in teams of 2-3 students**, so please connect with your classmates early.\n",
    "\n",
    "\n",
    "How you use the 100 labeled images for developing your system is up to you. You can use them to train a model, or simply use them to evaluate more advanced techniques, such as zero-shot learning or few-shot learning. You will have to do some research online to find suitable models and methods, since simple models taught in introductory deep learning courses won't perform well on this problem and dataset.\n",
    "\n",
    "\n",
    "\n",
    "At the end of this assignment, you need to write a three or more pages PDF report describing the methods you used to solve the assignment and other details outlined later in the assignment.\n",
    "\n",
    "In summary, your tasks are as follows:\n",
    "1. Form a group of 2-3 students\n",
    "2. Complete data preparation code\n",
    "3. Complete evaluation code to assess your models and methods\n",
    "4. Explore and implement deep learning based methods (models) to perform classification on the given dataset\n",
    "5. Evaluate your method on the 100 labeled training samples\n",
    "6. Standardize your prediction into a given format\n",
    "7. Update github multiple times\n",
    "8. Evaluate your model on the 902 unlabeled images in the test set by submitting your predictions to Kaggle to participate in the in-class competition\n",
    "9. Write a report of your work\n",
    "\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> For more information about the dataset, how to approach this problem, grading criteria, and what to include in the report, please read through the previous and later sections of the assignment.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> When implementing the provided functions, please use the doc-strings as reference to what we expect from your implementation.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> Claim authorship of code developed on your own, by adding  several comment lines at the begining of a file. Comment your own code thoroughly and include links and citations to external material. When using online code, add comments which demonstrate understanding of the code modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a11fd2",
   "metadata": {
    "id": "33aba3f5-832b-40fa-a4bf-cfc814d05130",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Downloading the Dataset\n",
    "Download the dataset from the \"E6691.2025Spring.Assignment2.Data\" folder in the [root directory of the class Google Drive](https://drive.google.com/drive/folders/1tluDzQ-Fjvf7wXC_xWwIkpzsqBgheQnd?usp=sharing).\n",
    "\n",
    "If you are using GCP (recommended), you will have to transfer the data over to your GCP instance.\n",
    "\n",
    "After downloading the dataset, you should store the zipped file in the provided `data` folder in this repository. Here, you can unzip the file, which contains the following folder and files:\n",
    "```  \n",
    "├── 2ndFloorData\n",
    "│   ├── images\n",
    "│   │   ├── L2ndFloor-D-2022-04-10_T-00_30_02.jpg\n",
    "│   │   └── ...\n",
    "│   ├── provided_labels.json\n",
    "│   └── classes.json\n",
    "```  \n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> **DO NOT PUSH YOUR DATA TO YOUR GitHub REPOSITORY AND DO NOT SUBMIT THE DATASET TO GRADESCOPE**\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> **YOU CANNOT SHARE THIS DATASET PUBLICLY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb477",
   "metadata": {
    "id": "5f04aa14-1d5f-4c48-8a2d-a9eae62a82f0"
   },
   "source": [
    "## Data Setup (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da24278",
   "metadata": {
    "id": "661ed4a1-32f1-45a5-a90c-dcfd08f330c5"
   },
   "source": [
    "Before we can do analysis or processing, we need to setup and load the data. We have provided three functions to load the data and to split it into two sets: (1) training, (2) testing. Note that the \"training\" set is treated more as a \"development\" set, as you might not necessarily be training any models, but only use the samples for developing and testing your models.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Complete  functions `load_image_data`, `load_labels_and_classes`, and  `split_images_into_train_test` in `utils/data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "258bce3b",
   "metadata": {
    "id": "3c9b915d-fe94-46d8-a54a-0419ccff6a14"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f0d61b",
   "metadata": {
    "id": "79891c5a-ca52-441c-af31-93f27fea0c91",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images path loaded: 1002\n",
      "Number of images with label: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/utils\")\n",
    "from data import load_image_data, load_labels_and_classes, split_images_into_train_test, sort_labels\n",
    "os.chdir(\"/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410\")\n",
    "\n",
    "\n",
    "IMAGE_DIR = \"2ndFloorData/images\"\n",
    "LABEL_FILE = \"2ndFloorData/provided_labels.json\"\n",
    "CLASSES_FILE = \"2ndFloorData/classes.json\"\n",
    "\n",
    "image_file_paths = load_image_data(IMAGE_DIR)\n",
    "labels, classes = load_labels_and_classes(LABEL_FILE, CLASSES_FILE)\n",
    "labels = sort_labels(labels)\n",
    "\n",
    "#train_images, test_images = split_images_into_train_test(image_file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d4945c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of test samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_images)}\")\n",
    "print(f\"Number of test samples: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3f70e",
   "metadata": {
    "id": "f87de582-74c2-4022-a1c5-91ab0aab4c5e"
   },
   "source": [
    "## Complete the Evaluation Code (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65961770",
   "metadata": {
    "id": "a830249f-7d20-489b-9512-e1914247fb6d"
   },
   "source": [
    "Since we are dealing with highly imbalanced classes, we will evaluate our methods using the [balanced accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) defined as the average of recall obtained from each class\n",
    "\n",
    "$$\n",
    "\\text{Balanced Accuracy} = \\frac{1}{N} \\sum_{i=1}^N \\frac{TP_i}{TP_i + FN_i}\n",
    "$$\n",
    "where $N$ is the number of classes, $TP_i$ is the number of true positive for class $i$ and $FN_i$ is the number of false negative for class $i$.\n",
    "\n",
    "However, since we're working with a multi-label classification problem, we cannot apply this metric naively. Instead, we treat each class as a binary classification problem (with $N=2$) with value 1 if the class is present in the image and 0 otherwise. We then compute the balanced accuracy for each class and compute the mean balanced accuracy across all classes to get a single aggregate score\n",
    "$$\n",
    "\\text{Mean Balanced Accuracy} = \\frac{1}{C} \\sum_{j=1}^C \\text{Balanced Accuracy}_j\n",
    "$$\n",
    "where $C$ is the number of classes we have in our dataset.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Complete  functions `convert_labels_to_multi_hot_encoding` in `utils/data.py` and `compute_balanced_accuracy` in `utils/metrics.py`. You will run `compute_balanced_accuracy` later in the assignment after implementing your own prediction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8e2da0",
   "metadata": {
    "id": "7a1959e4-4a43-462c-a2e9-792fa702792a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/utils\")\n",
    "from data import convert_labels_to_multi_hot_encoding\n",
    "\n",
    "# Convert labels dict to multi-hot encoding\n",
    "labels_multi_hot_encoding = convert_labels_to_multi_hot_encoding(labels, classes)\n",
    "labels_multi_hot_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['L2ndFloor-D-2021-12-22_T-22_00_01']\n",
    "labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ad3a4",
   "metadata": {},
   "source": [
    "# Test Reconigze Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac88c60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs4613/.conda/envs/recognize-anything/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rs4613/.conda/envs/recognize-anything/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/rs4613/.conda/envs/recognize-anything/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything\")\n",
    "from inference_ram_plus_openset_modified import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d862abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "pretrained/ram_plus_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from pretrained/ram_plus_swin_large_14m.pth\n",
      "vit: swin_l\n",
      "Building tag embedding:\n",
      "Creating pretrained CLIP model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 29.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import inference_ram_openset as inference\n",
    "from ram import get_transform\n",
    "\n",
    "from ram.utils import build_openset_llm_label_embedding\n",
    "from torch import nn\n",
    "import json\n",
    "\n",
    "image_size = 384\n",
    "pretrained = 'pretrained/ram_plus_swin_large_14m.pth'\n",
    "llm_tag_des = 'datasets/openimages_rare_200/openimages_rare_200_llm_tag_descriptions.json'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = get_transform(image_size=image_size)\n",
    "\n",
    "#######load model\n",
    "model = ram_plus(pretrained=pretrained,\n",
    "                         image_size=image_size,\n",
    "                         vit='swin_l')\n",
    "\n",
    "#######set openset interference\n",
    "\n",
    "print('Building tag embedding:')\n",
    "with open(llm_tag_des, 'rb') as fo:\n",
    "    llm_tag_des = json.load(fo)\n",
    "openset_label_embedding, openset_categories = build_openset_llm_label_embedding(llm_tag_des)\n",
    "\n",
    "model.tag_list = np.array(openset_categories)\n",
    "\n",
    "model.label_embed = nn.Parameter(openset_label_embedding.float())\n",
    "\n",
    "model.num_class = len(openset_categories)\n",
    "# the threshold for unseen categories is often lower\n",
    "model.class_threshold = torch.ones(model.num_class) * 0.5\n",
    "#######\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73507d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tag_to_label(tag, classes):\n",
    "    if not tag:  # More Pythonic way to check if tag is empty\n",
    "        return np.zeros(14)\n",
    "\n",
    "    # Use set for faster lookup and strip + lower in one go\n",
    "    tag_set = {item.strip().lower() for item in tag.split(\"|\")}\n",
    "    \n",
    "    # Create one-hot label with list comprehension\n",
    "    one_hot_label = np.array([1 if class_obj in tag_set else 0 for class_obj in classes.values()])\n",
    "    \n",
    "    return one_hot_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8b55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Slicing(img, window_height, window_width, stride):\n",
    "    img_height, img_width, num_channels = img.shape\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    \n",
    "    slices = []\n",
    "    for i in range(0, img_height - window_height + 1, int(stride * window_height)):\n",
    "        for j in range(0, img_width - window_width + 1, int(stride * window_width)):\n",
    "            img_segment = img[i:i+window_height, j:j+window_width,:]\n",
    "            slices.append(img_segment)\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d6e8d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ndFloorData/images/L2ndFloor-D-2022-01-29_T-22_00_01.jpg ['', 'Construction worker', '', '', '', '', '', '', 'Police car', 'Police car', '', 'Traffic cone', 'Police car', 'Construction worker', '', 'Construction worker', '', '', '', '', '', '', '', '', 'Police car', 'Police car', '', '', '', 'Construction worker', '', '', '', 'Traffic cone', 'Traffic cone', '', '', '', 'Barricade | Traffic cone | Traffic barrel | Police car | Construction truck | Car moving truck', 'Barricade | Traffic cone | Traffic barrel | Police car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade', '', '', '', '', 'Traffic cone | Traffic barrel', 'Traffic cone | Traffic barrel', 'Trailer truck | Construction truck | Car moving truck | Construction car', 'Traffic cone', 'Barricade | Traffic cone | Construction car | Construction worker', 'Traffic cone | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Construction car | Construction worker', '', '', '', '', 'Traffic cone | Construction worker', 'Traffic cone | Traffic barrel | Construction worker', 'Traffic cone | Construction truck | Construction car | Construction worker', 'Construction truck | Construction car | Construction worker', 'Traffic cone | Construction truck | Construction car | Construction worker', 'Traffic cone | Construction truck | Construction car | Construction worker', '', 'Traffic cone | Construction car | Construction worker', 'Barricade', 'Barricade | Scaffold | Trailer truck | Police car | Construction truck | Car moving truck | Construction car | Construction worker', '', '', '', '', '', '', 'Construction worker', 'Construction truck | Construction car', 'Excavator | Construction truck | Construction car', 'Excavator | Construction truck | Offroad parking | Construction car', '', 'Traffic cone', 'Traffic cone', 'Construction car | Construction worker', '', '', '', '', '', '', '', '', '', '', '', 'Construction car', '', '']\n",
      "2ndFloorData/images/L2ndFloor-D-2022-01-07_T-14_00_02.jpg ['', '', 'Trailer truck | Construction truck | Car moving truck | Construction car', 'Trailer truck | Construction truck | Car moving truck | Construction car', 'Police car', '', '', '', '', '', '', '', 'Police car', '', 'Construction worker', 'Scaffold | Construction worker', 'Scaffold | Construction truck | Construction car | Construction worker', 'Scaffold | Construction truck | Construction worker', 'Police car | Construction truck | Construction car', '', '', '', 'Car moving truck', '', '', '', '', '', '', 'Scaffold | Construction worker', 'Scaffold | Construction truck | Construction car | Construction worker', 'Barricade | Scaffold | Police car | Construction worker', '', 'Traffic cone | Construction car', '', '', '', 'Construction car', '', 'Traffic cone', 'Barricade | Traffic cone | Police car', 'Barricade', '', '', '', 'Trailer truck | Police car | Ambulance | Firecar | Construction truck | Car moving truck | Offroad parking | Construction car', 'Trailer truck | Police car | Ambulance | Construction truck | Car moving truck', '', 'Traffic cone', '', '', '', '', 'Barricade | Traffic cone | Traffic barrel | Police car', 'Barricade | Traffic cone | Traffic barrel | Police car | Construction car', 'Barricade | Traffic cone', '', '', '', 'Police car | Ambulance | Firecar | Car moving truck', 'Traffic cone | Trailer truck | Police car | Ambulance | Firecar | Construction truck | Car moving truck | Construction car', 'Traffic cone', 'Traffic cone', 'Traffic cone', '', 'Traffic cone', '', 'Barricade | Traffic cone', 'Barricade | Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Construction worker', '', '', '', 'Police car | Car moving truck', 'Traffic cone | Trailer truck | Police car | Car moving truck | Construction car', '', '', 'Traffic cone', 'Traffic cone', '', '', 'Traffic cone', '', 'Barricade | Traffic cone | Police car', '', '', '', '', '', '', '', '', 'Traffic cone | Construction worker', 'Excavator | Construction truck | Construction car | Construction worker', '', '', '', '']\n",
      "2ndFloorData/images/L2ndFloor-D-2022-04-09_T-18_30_02.jpg ['', 'Construction worker', '', 'Police car | Ambulance', 'Police car', 'Barricade | Scaffold | Construction car | Construction worker', 'Barricade | Scaffold | Construction car | Construction worker', 'Barricade | Scaffold | Construction worker', 'Scaffold | Police car | Construction worker', 'Scaffold | Police car', 'Scaffold | Police car', 'Police car', 'Police car', '', 'Construction worker', 'Scaffold | Construction worker', 'Barricade | Scaffold | Construction worker', 'Barricade | Police car | Ambulance | Car moving truck', 'Police car | Ambulance', 'Barricade | Scaffold | Construction car | Construction worker', 'Barricade | Scaffold | Construction car | Construction worker', 'Barricade | Scaffold | Construction worker', 'Barricade | Traffic cone | Scaffold | Police car | Construction worker', 'Barricade | Traffic cone | Police car', 'Barricade | Traffic cone | Traffic barrel | Police car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Police car | Ambulance | Construction worker', '', '', '', 'Scaffold | Construction worker', 'Barricade | Scaffold | Construction worker', 'Police car', '', '', '', '', 'Traffic cone | Police car', 'Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Police car | Ambulance | Construction worker', 'Barricade | Traffic cone | Police car | Construction worker', 'Barricade', '', '', '', '', '', 'Traffic cone', '', '', 'Police car', 'Traffic cone', '', 'Barricade | Police car', 'Barricade | Traffic cone | Police car | Construction car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Scaffold | Construction worker', '', '', '', '', '', 'Traffic cone', 'Police car', '', 'Police car', 'Traffic cone | Police car', 'Traffic cone | Police car', 'Traffic cone | Police car', 'Barricade | Traffic cone | Police car', 'Barricade | Traffic cone | Traffic barrel | Scaffold | Police car | Ambulance | Construction car | Construction worker', '', '', '', '', '', 'Traffic cone', 'Police car', 'Police car', '', 'Police car', '', 'Traffic cone', 'Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Police car', '', '', '', '', '', '', '', 'Police car', 'Police car', '', '', 'Traffic cone', 'Traffic cone', 'Traffic cone | Police car']\n",
      "2ndFloorData/images/L2ndFloor-D-2022-02-14_T-14_30_01.jpg ['', 'Construction worker', 'Barricade | Trailer truck | Construction truck | Car moving truck | Construction car', 'Trailer truck | Construction truck | Car moving truck | Construction car', '', 'Barricade | Scaffold | Trailer truck | Excavator | Construction truck | Car moving truck | Construction car | Construction worker', 'Barricade | Scaffold | Trailer truck | Construction truck | Car moving truck | Construction car | Construction worker', 'Barricade | Scaffold | Construction truck | Construction car | Construction worker', 'Barricade | Traffic cone | Scaffold | Construction truck | Construction car | Construction worker', 'Scaffold | Police car | Construction car | Construction worker', 'Scaffold', 'Police car', 'Police car', 'Police car | Construction worker', 'Construction worker', '', 'Barricade | Scaffold | Trailer truck | Construction truck | Car moving truck | Construction car | Construction worker', 'Barricade | Scaffold | Trailer truck | Construction truck | Car moving truck | Construction car | Construction worker', '', 'Barricade | Scaffold | Trailer truck | Construction truck | Car moving truck | Construction car | Construction worker', 'Barricade | Scaffold | Trailer truck | Construction truck | Car moving truck | Construction car | Construction worker', 'Barricade | Traffic cone | Scaffold | Construction truck | Construction car | Construction worker', 'Scaffold | Construction worker', 'Scaffold', '', '', '', '', '', 'Construction worker', 'Barricade | Scaffold | Construction worker', 'Barricade | Traffic cone | Scaffold | Construction truck | Construction car | Construction worker', '', '', 'Police car', '', 'Construction worker', '', '', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', '', '', '', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', '', '', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Scaffold | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Scaffold | Police car | Car moving truck | Construction car | Construction worker', '', '', '', '', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Police car | Car moving truck | Construction car | Construction worker', '', '', '', '', 'Traffic cone', '', '', 'Traffic cone', 'Traffic cone', '', '', 'Traffic cone', 'Traffic cone | Police car', 'Barricade | Traffic cone | Police car', '', 'Construction worker', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ndFloorData/images/L2ndFloor-D-2022-01-09_T-00_00_01.jpg ['', 'Police car', '', '', '', '', 'Police car | Ambulance | Firecar', 'Firecar', '', 'Police car', 'Police car', 'Police car | Ambulance | Firecar', 'Police car | Ambulance | Firecar', 'Police car | Ambulance | Firecar', '', '', '', '', 'Traffic cone | Police car', 'Police car', '', '', '', 'Police car', 'Police car | Ambulance', 'Police car | Ambulance', 'Police car | Ambulance', '', '', '', '', 'Barricade | Traffic cone | Construction worker', '', 'Police car', 'Police car', '', '', 'Police car | Construction truck | Construction car | Construction worker', 'Police car | Ambulance', 'Barricade | Traffic cone | Traffic barrel | Police car | Ambulance', 'Barricade | Traffic cone | Traffic barrel | Construction worker', 'Police car | Ambulance', '', '', '', 'Barricade | Traffic cone | Construction worker', 'Traffic cone | Traffic barrel', 'Traffic cone', '', '', '', '', 'Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Construction car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction car | Construction worker', 'Barricade | Traffic cone | Police car', '', '', '', '', '', '', '', '', '', '', '', 'Barricade | Traffic cone', 'Barricade | Traffic cone | Traffic barrel | Construction car | Construction worker', 'Barricade | Traffic cone | Traffic barrel | Construction car | Construction worker', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Barricade | Traffic cone | Traffic barrel', '', '', '', '', '', '', 'Traffic cone', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_file_paths:\n\u001b[0;32m----> 2\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image_path, tag)\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/inference_ram_plus_openset_modified.py:40\u001b[0m, in \u001b[0;36mget_labels\u001b[0;34m(model, transform, device, image)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sliced_image \u001b[38;5;129;01min\u001b[39;00m sliced_images:\n\u001b[1;32m     39\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(sliced_image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 40\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     res_list\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_list\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/inference.py:44\u001b[0m, in \u001b[0;36minference_ram_openset\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minference_ram_openset\u001b[39m(image, model):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m         tags \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_tag_openset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tags[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/models/ram_plus.py:347\u001b[0m, in \u001b[0;36mRAM_plus.generate_tag_openset\u001b[0;34m(self, image, threshold, tag_input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_tag_openset\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    342\u001b[0m              image,\n\u001b[1;32m    343\u001b[0m              threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.68\u001b[39m,\n\u001b[1;32m    344\u001b[0m              tag_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m              ):\n\u001b[0;32m--> 347\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    348\u001b[0m     image_atts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(image_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    349\u001b[0m                             dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(image\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    351\u001b[0m     image_cls_embeds \u001b[38;5;241m=\u001b[39m image_embeds[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/models/swin_transformer.py:569\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x, idx_to_group_img, image_atts, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 569\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# B L C\u001b[39;00m\n\u001b[1;32m    573\u001b[0m x_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# B C 1\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/models/swin_transformer.py:396\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    394\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x)\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 396\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/models/swin_transformer.py:256\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[1;32m    259\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/recognize-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/recognize-anything/ram/models/swin_transformer.py:123\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    x: input features with shape of (num_windows*B, N, C)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m B_, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 123\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for image_path in image_file_paths:\n",
    "    tag = get_labels(model, transform, device, f\"/home/rs4613/Class/EE6691/e6691-2025spring-assign2-2ndfloorimages-kb3343_rs4613_af3410/{image_path}\")\n",
    "    print(image_path, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f4609",
   "metadata": {
    "id": "0aa19b20-b559-40f8-95c4-456e28ebf723"
   },
   "source": [
    "## Explore and Build Your Own Model(s) (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3418a7",
   "metadata": {
    "id": "91d7f4da-4151-442a-9ba8-a9b400549d9e",
    "tags": []
   },
   "source": [
    "You need to build your own system(s) (pipelines/models/methods) to perform multi-label classification on the dataset you have prepared. Use the 100 labeled images as your development set to implement a pipeline to perform classification on the 902 unlabeled images. You will need to write your own custom code for this part, but you can use the methods you implemented in the previous part of the assignment.\n",
    "\n",
    "After developing your system, you will perform prediction on the 902 test images without labels, and submit your results to Kaggle after joining the class Kaggle competition (see the next sections for more details regarding the format of the results, how to submit your results, and how the results are graded). Try out several methods and submit as many times as you want (max 20 submissions per day).\n",
    "\n",
    "Unless you have a clear idea on how to tackle this problem from the get-go, this part will require you to do some research online to look for methods and models beyond what you typically explore in introductory deep learning classes.\n",
    "\n",
    "Provide a link or citations to every model you try.\n",
    "\n",
    "Here are some tips you can take into consideration when building your system. You do not have to follow every point, but the tips might be helpful to get you started:\n",
    "* <font color=\"red\"><strong>NOTE:</strong></font> Remember that you are dealing with a multi-label classification problem. Many models do not solve this problem out-of-the-box. You might need to think about how to design/adapt your models to predict several labels per image at the same time. However, some models do predict multiple classes at the same time, and in those cases, you might not have to do any modifications.\n",
    "* Since the labeled dataset is so small, techniques such as **Zero-shot learning** or **Few-shot learning** might be good choices for this task.\n",
    "* Try different types of models. Even though we are dealing with a classification task, (zero-shot) object detection models might still work well if used properly.\n",
    "* Also, feel free to look beyond just classification models. Maybe generative models could be of use?\n",
    "* Again, since the labeled dataset is small, training a typical CNN classifier with so few images might not be feasible. However, feel free to try to train (or fine-tune) some models and see what results you get, and  to go beyond  models you find on torchvision. E.g., you can look for other classification models on Huggingface.\n",
    "* Remember, the dataset is highly imbalanced. Naively training a classifier might cause the model to be extremely biased. If you decide to train anything, please be careful of this fact.\n",
    "* Furthermore, the images are of $4K$ resolution. Typical large-scale datasets are not this high-resolution, so naively feeding the images into a pre-trained model as is might not work well. Consider some of the following ideas:\n",
    "    * Can you resize the images before feeding them into the model?\n",
    "    * Can you perform some sort of sliding window approach?\n",
    "    * Is there any way to (automatically) focus on specific smaller parts of the image that might be more interesting than others for classification? If so, how can this be done?\n",
    "* You can try combining different models together into a single pipeline to see if that can help.\n",
    "* Communicate with others in your team.\n",
    "* Be creative!\n",
    "\n",
    "### Grading\n",
    "This part of the assignment is graded based on (weighted equally):\n",
    "* **Clarity of code** - Is the code well structured and commented? (This is also important for the final project)\n",
    "* **Correctness** - Does the model / whole pipeline solve the presented problem (multi-class classification)?\n",
    "* **Effort** - Have you put a genuine effort into the assignment? (e.g., a submission with five different poorly performing methods will be graded more favourably than one with a single method that did not perform well). We expect you to try out more advanced methods to score well here. I.e., simply loading and fine-tuning an out-of-the-box ResNet from torchvision will not be sufficient (unless you do modifications to it, or use it in a larger pipeline).\n",
    "    \n",
    "<font color=\"red\"><strong>TODO:</strong></font> Implement your custom pipeline in the cells below. Feel free to make as many new cells as you'd like. However, keep all the cells with relevant code between this current cell and the `## KEEP YOUR CODE ABOVE THIS CELL!` cell below.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> You must use PyTorch as your deep learning framework for this part of the assignment. The models you run must use PyTorch. However, you are free to combine it with other libraries, such as scikit-learn and transformers (Huggingface) if it is relevant.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> Clearly comment your code to make it easy to follow.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> Remember to give attribution to sources you use. Add links and citations to methods and models you use as comments when they are applied. Also include them in your report.  \n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> Feel free to try several different methods.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> If you find yourself stuck on this part, please come to TA's office hours to discuss approaches and implementations. We will be much more helpful in person than on EdStem for this assignment.\n",
    "\n",
    "<font color=\"red\"><strong>NOTE:</strong></font> START EARLY! This part of the assignment will take a lot of time to get right, and you most likely will not be able to complete it within 5-6 days only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d75d7",
   "metadata": {
    "id": "85d75cfd-79be-4ae7-aa3c-ef4a7d0aba5e"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# --------------------------- YOUR IMPLEMENTATION HERE ---------------------------- #\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# --------------------------- END YOUR IMPLEMENTATION ----------------------------- #\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b41a1",
   "metadata": {
    "id": "e08085c4-36ca-45cf-9cd2-c81535a9e765"
   },
   "outputs": [],
   "source": [
    "# Make more cells if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85747329",
   "metadata": {
    "id": "1d2ccb74-9a02-4d99-af10-2c668b429bc4"
   },
   "outputs": [],
   "source": [
    "# Make more cells if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7420544",
   "metadata": {
    "id": "a59a85b1-1331-4583-8ae4-5325aa2cc8c0"
   },
   "outputs": [],
   "source": [
    "# Make more cells if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a008b7",
   "metadata": {
    "id": "81a1aa57-002c-4270-808e-e375f962ef0c"
   },
   "outputs": [],
   "source": [
    "# Make more cells if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da355a",
   "metadata": {
    "id": "7a089fda"
   },
   "outputs": [],
   "source": [
    "# Your predictions here\n",
    "predictions ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35e61a",
   "metadata": {
    "id": "ded532e2-202b-4b00-8db9-7095d36fee8f"
   },
   "source": [
    "## KEEP YOUR CODE ABOVE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c8136",
   "metadata": {
    "id": "bdb2b474-635d-4f21-be3d-093bb7818e19"
   },
   "source": [
    "## Evaluate your method on the whole training set of 100 labeled images in the following cells (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b56ac1",
   "metadata": {
    "id": "79666ddd-b027-4772-ba75-408080a28177"
   },
   "source": [
    "<font color=\"red\"><strong>TODO 2:</strong></font> After completing your code, please evaluate your method on the whole training set of 100 labeled images in the following cell using `compute_balanced_accuracy` function in `utils/metrics.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4707e7e",
   "metadata": {
    "id": "07708d1f-8ec3-4618-9f38-3dada1bbba18"
   },
   "outputs": [],
   "source": [
    "from utils.metrics import compute_balanced_accuracy\n",
    "\n",
    "# Compute the balanced accuracy across all classes\n",
    "mean_balanced_accuracy, accuracies = compute_balanced_accuracy(predictions, labels_multi_hot_encoding)\n",
    "\n",
    "print(\"Mean Balanced Accuracy (Computed over all classes):\", mean_balanced_accuracy)\n",
    "print('Balanced Accuracy Per Class:')\n",
    "for class_idx, class_name in classes.items():\n",
    "    print(f'{class_name}: {accuracies[int(class_idx)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6cdae",
   "metadata": {
    "id": "4fd231b0-7871-4eb2-9d1f-c0489e40ef21"
   },
   "source": [
    "## Prepare for Kaggle - Convert predictions into desirable format (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce971fad",
   "metadata": {
    "id": "80d034c4-0004-4c46-85a3-0ada0abbcb68"
   },
   "source": [
    "The previous section of this assignment required you to explore and implement methods and models of your choice to solve the given problem. This freedom makes it hard for us to evaluate your models. Therefore, we need you to convert your predictions on the test set (the 902 images) into a uniform format for submission to Kaggle. This way, we can evaluate your models using a single script, and you can participate in the class-wide Kaggle competition.\n",
    "\n",
    "We want your predictions to be a CSV file with 15 columns. The first column should have `ID` as the header, while the other 14 columns should have 0 to 13 as headers. One row should represent the predictions for one image, and the `ID` for each image is simply the filename of the image. The class should map to the integer found in `classes.json`. I.e., `barricade` should have label `0`, `traffic cone` should be `1`, etc. Each of the 14 columns (with headers 0 to 13) should represent whether a class is predicted or not, with value 0 representing not present, and value 1 representing present.\n",
    "\n",
    "\n",
    "An example of the correct format is as follows. If for image `L2ndFloor-D-2021-12-31_T-00_00_01`, we predict the presence of `barricade` (class 0), `traffic barrel` (class 2), `scaffold` (class 3), and `police car` (class 5), the CSV should be as follows:\n",
    "```\n",
    "ID, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n",
    "L2ndFloor-D-2021-12-31_T-00_00_01, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0\n",
    "```\n",
    "And if we additionally have the predictions 0, 2, 3 for `L2ndFloor-D-2021-12-27_T-22_30_01`, we would simply add another row:\n",
    "```\n",
    "ID, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n",
    "L2ndFloor-D-2021-12-31_T-00_00_01, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0\n",
    "L2ndFloor-D-2021-12-27_T-22_30_01, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "...\n",
    "...\n",
    "...\n",
    "```\n",
    "You should have 902 rows in your CSV file, each representing one of the 902 images.\n",
    "\n",
    "This should then be saved as a .csv file and uploaded to Kaggle following the instructions in the next section of the assignment.\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Complete the following function in the cell below to (1) convert your predictions into the desired format described above for Kaggle submission and (2) save the predictions to a .csv file.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> When completing the functions, please fill out the doc strings where we have marked with TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ba18d",
   "metadata": {
    "id": "785b0f63-db5e-49c7-84d7-04f4fa0caae1"
   },
   "outputs": [],
   "source": [
    "def convert_predictions_to_kaggle_format_and_save_to_csv(a,b,c,d, save_dir):\n",
    "    \"\"\"\n",
    "    Converts your predictions into the desirable format as described above.\n",
    "    You should save your predictions as a .csv file.\n",
    "\n",
    "    args:\n",
    "        TODO: After changing a,b,c,d to something meaningful that matches your predictions, describe your arguments!\n",
    "        NOTE: One of the a,b,c,d\n",
    "        TODO: Remove these two TODOs and the NOTE and replace it with your own args.\n",
    "        HOWEVER: Keep the save_dir\n",
    "\n",
    "        save_dir: Where to save the .csv file.\n",
    "\n",
    "\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################################################\n",
    "    # --------------------------- YOUR IMPLEMENTATION HERE ---------------------------- #\n",
    "    #####################################################################################\n",
    "\n",
    "    #TODO: Complete this code\n",
    "    # You need to change the arguments of the function. E.g., replace a,b,c,d with proper variable names\n",
    "\n",
    "    raise Exception('convert_predictions_to_kaggle_format_and_save_to_csv not implemented!') # delete me\n",
    "\n",
    "    #####################################################################################\n",
    "    # --------------------------- END YOUR IMPLEMENTATION ----------------------------- #\n",
    "    #####################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366fbcc",
   "metadata": {
    "id": "ccbac85b-2f29-4220-9477-4f36306f35f5"
   },
   "source": [
    "## Evaluate your model (Kaggle) (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100baa09",
   "metadata": {
    "id": "7f55e06d-4861-41dd-8588-3d767d9096d6"
   },
   "source": [
    "With your predictions ready, you can now participate in the in-class Kaggle competition. This part does not contain any explicit coding, but only details regarding the Kaggle competition.\n",
    "\n",
    "Kaggle is a platform for predictive modeling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to sign up at www.kaggle.com with your **Columbia EMAIL**. To let the TAs do the grading more conveniently, again, please use **Lionmail** to join Kaggle and make your team name a combination of all the group members UNI's: `UNI1-UNI2-UNI3` for your group to submit together.\n",
    "\n",
    "The competition is located here: https://www.kaggle.com/t/608219c156cd48efa18ce7955c8ee07b\n",
    "\n",
    "You can find detailed description about this in-class competition on the website above. Please read and follow the instructions carefully.\n",
    "\n",
    "Your task is to submit the .csv file with predictions from the previous section to the Kaggle competition website. Your performance will be shown on the public leaderboard once you submit your prediction .csv file. The final ranking is based on the public leaderboard results.\n",
    "\n",
    "You will be graded based on your results, so please try your best to get as high of a score as possible!\n",
    "\n",
    "More specifically, you can receive between 0 and 10 points for this part of the assignment according to the following table:\n",
    "| Balanced Accuracy| Grade points |\n",
    "| :--: | :--: |\n",
    "| 56-  | 0   |\n",
    "| 56 - 57  | 1   |\n",
    "| 57 - 58  | 2 |\n",
    "| 58 - 59  | 3 |\n",
    "| 59 - 60  | 4 |\n",
    "| 60 - 61  | 5 |\n",
    "| 61 - 62  | 6 |\n",
    "| 62 - 63  | 7 |\n",
    "| 63 - 64  | 8 |\n",
    "| 64 - 65  | 9 |\n",
    "| 65 +  | 10 |\n",
    "\n",
    "\n",
    "\n",
    "Feel free to try out several methods and submit as many times as you want (max 20 submissions per day).\n",
    "\n",
    "Kaggle team name **MUST** be `UNI1-UNI2-UNI3`\n",
    "\n",
    "<font color=\"red\"><strong>Note: </strong></font> You cannot change group after joining one - Kaggle won't let you.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> [**REPLACE THIS WITH YOUR KAGGLE TEAM NAME**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31fe14",
   "metadata": {
    "id": "8efe743e-f77d-44fa-a23a-9baac030124f"
   },
   "source": [
    "## Write a report (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152fba7",
   "metadata": {
    "id": "f1e7a1b6-d082-4ef5-b943-681811805902"
   },
   "source": [
    "Please write a 3 or more pages PDF report containing the following information:\n",
    "1. Explanation and motivation for your approach\n",
    "    * You need to discuss the following:\n",
    "        * Examine some samples from the training set. What insights might be useful to develop your approach?\n",
    "        * Which of the provided information about the dataset did you consider when choosing your method?\n",
    "    * Please describe the methods and models you used, and include citations of work found online\n",
    "    * Any other interesting insights that motivated your approach\n",
    "2. Include a table summarizing the results from evaluations on the training set\n",
    "    * These are the results from the `Evaluate your method on the whole training set of 100 labeled images in the following cells` section.\n",
    "    * In your table, include the balanced accuracy for each class, and the mean balanced accuracy over all classes.\n",
    "        * One row should represent one class or the mean balanced accuracy over all classes.\n",
    "        * Optional, but you can also include rows summarizing the performance over a subset of classes.\n",
    "        * One column should represent one model.\n",
    "3. Discuss your results on the training set. You can, among other points, discuss the following:\n",
    "    * What insights did you gain?\n",
    "    * Are some classes easier to predict than others? Why do you think so?\n",
    "    * Why are some classes harder than others?\n",
    "    * How can you use these insights to improve your model/pipeline?\n",
    "        * Did you implement these improvements? If so, how? And how did it affect your results?\n",
    "4. Include your best resulting score (mean balanced accuracy) that you achieved from the Kaggle competition\n",
    "5. Based on your results your Kaggle score, discuss your results and potential improvement strategies\n",
    "    * Why do you think your model performed well / subpar?\n",
    "    * Are the results similar to those from the training set?\n",
    "    * Are there any insights from the training set that could have been the cause of your performance?\n",
    "    * What could you do to improve your model and the performance?\n",
    "6. Contribution by each student.\n",
    "    * Add a table summarizing the contribution of each student and the percentage of work each student did.\n",
    "\n",
    "The report PDF should be included in your GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f55c8",
   "metadata": {
    "id": "9f2a1e8b-5a73-48ef-9bc0-fe513f34b20b"
   },
   "source": [
    "## Final Submission (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939b75d",
   "metadata": {
    "id": "7d5d20d0-e8e8-41bb-8851-78e97ae54ec6"
   },
   "source": [
    "Finally, please carefully follow the submission instruction in the README.md.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Commit your changes regularly. <span style=\"color:red\">***At least 3***</span> commits by each student on your progress of the assignment is required before submitting a final version.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Remember to include two PDFs in your GitHub repository (and make sure they're submitted to Gradescope)\n",
    "1. PDF of this notebook\n",
    "2. PDF of your report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50709b6b",
   "metadata": {
    "id": "223643ae-8995-49cd-b914-50421c2dedce"
   },
   "source": [
    "# Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
